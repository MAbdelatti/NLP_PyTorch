{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using PyTorch for Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Info\n",
    "\n",
    "### 1. What is PyTorch?\n",
    "\n",
    "PyTorch is a free, open-source deep learning framework, based on the Torch library. It is used for applications such as natural language processing (NLP) and Computer Vision (CV). It is primarily developed by Facebook's artificial-intelligence research group.\n",
    "\n",
    "PyTorch provides two high-level features:\n",
    "\n",
    "1. Tensor computing (like NumPy) with strong acceleration via graphics processing units (GPU)\n",
    "2. Deep neural networks built on a tape-based autodiff system\n",
    "\n",
    "### 2. Pros of PyTorch over Tensorflow\n",
    "\n",
    "The main advantage of PyTorch over Tensorflow is that it uses dynamic computation graphs while Tensorflow (or Keras, or Theano) uses static graphs. In Tensorflow, computation graphs must be defined all at once and for all. Whereas in PyTorch graphs are inherited from participating tensors.\n",
    "\n",
    "A tensorflow program cannot be stopped using control flow at any point for debugging. In PyTorch, standard (if-else, loops) can be used.\n",
    "\n",
    "### 3. Installation\n",
    "\n",
    "The best resource to install PyTorch is by following the instruction provided at its official website: https://pytorch.org/. However, a brief steps to follow are explained below. To install it directly using pip library manager for Python 3.6 and CUDA 9.0, run the following command(s) in the command line:\n",
    "\n",
    "#### For Ubuntu Linux\n",
    "\n",
    "```bash\n",
    "pip3 install torch torchvision\n",
    "```\n",
    "#### For Windows\n",
    "``` bash\n",
    "pip3 install https://download.pytorch.org/whl/cu90/torch-1.1.0-cp36-cp36m-win_amd64.whl ```\n",
    "\n",
    "``` bash\n",
    "pip3 install https://download.pytorch.org/whl/cu90/torchvision-0.3.0-cp36-cp36m-win_amd64.whl ```\n",
    "\n",
    "### 4. Verfication\n",
    "\n",
    "To ensure that PyTorch was installed correctly, we can run a sample code that randomly initializes a tensor:\n",
    "``` python\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "x = torch.rand(5, 3)\n",
    "print(x) ```\n",
    "\n",
    "Additionally, to check if the GPU driver and CUDA are enabled and accessible by PyTorch, run the following code:\n",
    "``` python\n",
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "```\n",
    "This should return ``` True ```."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Example: Logistic Regression Bag-of-Words classifier\n",
    "\n",
    "This program reads an input text and outputs a probability of being an English or Spanish text. We first prepare the training and test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'me': 0, 'gusta': 1, 'comer': 2, 'en': 3, 'la': 4, 'cafeteria': 5, 'Give': 6, 'it': 7, 'to': 8, 'No': 9, 'creo': 10, 'que': 11, 'sea': 12, 'una': 13, 'buena': 14, 'idea': 15, 'is': 16, 'not': 17, 'a': 18, 'good': 19, 'get': 20, 'lost': 21, 'at': 22, 'Yo': 23, 'si': 24, 'on': 25}\n"
     ]
    }
   ],
   "source": [
    "# Code sample is brought from https://pytorch.org/tutorials/\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "data = [(\"me gusta comer en la cafeteria\".split(), \"SPANISH\"),\n",
    "        (\"Give it to me\".split(), \"ENGLISH\"),\n",
    "        (\"No creo que sea una buena idea\".split(), \"SPANISH\"),\n",
    "        (\"No it is not a good idea to get lost at sea\".split(), \"ENGLISH\")]\n",
    "\n",
    "test_data = [(\"Yo creo que si\".split(), \"SPANISH\"),\n",
    "             (\"it is lost on me\".split(), \"ENGLISH\")]\n",
    "\n",
    "# word_to_ix is a dictionary of the words in the vocab and its corresponding unique integer index,\n",
    "# which will be used in the Bag of words vector'''\n",
    "\n",
    "word_to_ix = {}\n",
    "for sent, _ in data + test_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "            \n",
    "print(word_to_ix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then , we define the classifier class which inherits from torch.nn.Module and torch.nn.Linear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the BoW classifier Class\n",
    "class BoWClassifier(nn.Module):  # inheriting from nn.Module!\n",
    "\n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "        # calls the init function of nn.Module.  Dont get confused by syntax,\n",
    "        # just always do it in an nn.Module\n",
    "        super(BoWClassifier, self).__init__()\n",
    "\n",
    "        # Define the parameters that you will need.  In this case, we need A and b,\n",
    "        # the parameters of the affine mapping. Where A is the weight matrix and b is the bias vector\n",
    "        # Torch defines nn.Linear(), which provides the affine map.\n",
    "        # This is equal to y = Ax + b\n",
    "        self.linear = nn.Linear(vocab_size, num_labels)\n",
    "\n",
    "    def forward(self, bow_vec):\n",
    "        # Pass the input through the linear layer,\n",
    "        # then pass that through log_softmax.\n",
    "        # Many non-linearities and other functions are in torch.nn.functional\n",
    "        return F.log_softmax(self.linear(bow_vec), dim=1)\n",
    "\n",
    "# Defining Some Functions\n",
    "def make_bow_vector(sentence, word_to_ix):\n",
    "    vec = torch.zeros(len(word_to_ix))\n",
    "    for word in sentence:\n",
    "        vec[word_to_ix[word]] += 1\n",
    "    return vec.view(1, -1)\n",
    "\n",
    "\n",
    "def make_target(label, label_to_ix):\n",
    "    return torch.LongTensor([label_to_ix[label]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we create an instance of the classifier with a specific vocabulary size and two labels (English and Spanish). The model knows its parameters. The first output below is A, the second is b. This is inherited from the torch.nn.Linear module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[ 0.1011, -0.0866, -0.0380,  0.0921, -0.1846,  0.1176, -0.0403,  0.0998,\n",
      "          0.0273, -0.0240,  0.0544,  0.0097,  0.0716, -0.0764, -0.0143, -0.0177,\n",
      "          0.0284, -0.0008,  0.1714,  0.0610, -0.0730, -0.1184, -0.0329, -0.0846,\n",
      "         -0.0628,  0.0094],\n",
      "        [ 0.1169,  0.1066, -0.1917,  0.1216,  0.0548,  0.1860,  0.1294, -0.1787,\n",
      "         -0.1865, -0.0946,  0.1722, -0.0327,  0.0839, -0.0911,  0.1924, -0.0830,\n",
      "          0.1471,  0.0023, -0.1033,  0.1008, -0.1041,  0.0577, -0.0566, -0.0215,\n",
      "         -0.1885, -0.0935]], requires_grad=True) \n",
      "\n",
      "Parameter containing:\n",
      "tensor([ 0.1064, -0.0477], requires_grad=True) \n",
      "\n",
      "--------------\n",
      "\n",
      "tensor([[-0.6250, -0.7662]])\n",
      "tensor([[-0.5870, -0.8119]])\n",
      "Before training: tensor([0.0544, 0.1722], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = len(word_to_ix)\n",
    "NUM_LABELS = 2\n",
    "\n",
    "model = BoWClassifier(NUM_LABELS, VOCAB_SIZE)\n",
    "\n",
    "for param in model.parameters():\n",
    "    print(param,'\\n')\n",
    "    \n",
    "print('--------------\\n')\n",
    "\n",
    "# To run the model, pass in a BoW vector\n",
    "label_to_ix = {\"SPANISH\": 0, \"ENGLISH\": 1}\n",
    "\n",
    "# Run on test data before we train, just to see a before-and-after\n",
    "with torch.no_grad():\n",
    "    for instance, label in test_data:\n",
    "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
    "        log_probs = model(bow_vec)\n",
    "        print(log_probs)\n",
    "\n",
    "# Print the matrix column corresponding to \"creo\"\n",
    "E_label_before = round(next(model.parameters())[:, word_to_ix[\"creo\"]][1].item(), 4)\n",
    "S_label_before = round(next(model.parameters())[:, word_to_ix[\"creo\"]][0].item(), 4)\n",
    "print('Before training:', next(model.parameters())[:, word_to_ix[\"creo\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So lets train! To do this, we forward-pass instances through to get log probabilities, compute a loss function. Then, we compute the gradient of the loss function, update the parameters in the backpropagation step. Loss functions are provided in the torch.nn module. Optimization functions are available in torch.optim. Here, we will just use the stochastic gradient descent (SGD) with a learning rate $\\eta=0.1$.\n",
    "\n",
    "Note that the input to NLLLoss is a vector of log probabilities, and a target label. It doesnâ€™t compute the log probabilities for us. This is why the last layer of our network is log softmax. The loss function nn.CrossEntropyLoss() is the same as NLLLoss(), except it does the log softmax for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# Usually you want to pass over the training data several times.\n",
    "# 100 is much bigger than on a real data set, but real datasets have more than\n",
    "# two instances.  Usually, somewhere between 5 and 30 epochs is reasonable.\n",
    "\n",
    "for epoch in range(100):\n",
    "    for instance, label in data:\n",
    "        \n",
    "        # Step 1. Remember that PyTorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Make our BOW vector and also we must wrap the target in a\n",
    "        # Tensor as an integer. For example, if the target is SPANISH, then\n",
    "        # we wrap the integer 0. The loss function then knows that the 0th\n",
    "        # element of the log probabilities is the log probability\n",
    "        # corresponding to SPANISH\n",
    "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
    "        target = make_target(label, label_to_ix)\n",
    "\n",
    "        # Step 3. Run the feed forward and compute the loss.\n",
    "        log_probs = model(bow_vec)\n",
    "        loss = loss_function(log_probs, target)        \n",
    "\n",
    "        # Step 4. Compute the gradients, and update the parameters by\n",
    "        # calling optimizer.step()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our model using the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1210, -2.1721]])\n",
      "tensor([[-2.7767, -0.0643]])\n",
      "After training: tensor([ 0.5004, -0.2738], grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for instance, label in test_data:\n",
    "        bow_vec = make_bow_vector(instance, word_to_ix)\n",
    "        log_probs = model(bow_vec)\n",
    "        print(log_probs)\n",
    "\n",
    "# Index corresponding to Spanish goes up, English goes down!\n",
    "E_label_after = round(next(model.parameters())[:, word_to_ix[\"creo\"]][1].item(),4)\n",
    "S_label_after = round(next(model.parameters())[:, word_to_ix[\"creo\"]][0].item(),4)\n",
    "print('After training:', next(model.parameters())[:, word_to_ix[\"creo\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "variables": {
     "E_label_after": "-0.2738",
     "E_label_before": "0.1722",
     "S_label_after": "0.5004",
     "S_label_before": "0.0544"
    }
   },
   "source": [
    "As seen from the output, the classification output for \"Spanish\" has been increased from {{S_label_before}} to {{S_label_after}}. Whereas, the classification output for \"English\" has been dereased from {{E_label_before}} to {{E_label_after}}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
